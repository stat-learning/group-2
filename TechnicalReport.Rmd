---
title: "Technical Report: Predicting Yelp Price Ranges"
author:
-"Josh Dey"
-"Emmett Powers"
-"Giorlando Ramirez"
date: "12/10/2019"
output: pdf_document
---

Technical Report
Your technical report should be an .Rmd file that contains the following sections. So as not to make the compilation (knitting) of the document not take too long, consider setting cache = TRUE in the curly braces of any R chunk with substantial computing. Please knit both to pdf and github document (.md).


# Abstract
A brief overview of the area that you’ll be investigating, the research question(s) of interest, your approach to analysis, and the general conclusions.

In this project, we leverage a large Yelp data set comprised of about 200k observations in order to build a predictive model of restaurant prices. We go about answering this question deploying three supervised learning models: linear regression, ordinal regression, and classification trees. We find that the available variables have fairly limited predictive power across our models.



# Introduction
Overview of the setting of the data, existing theories/models (particularly if you are working in a descriptive/inferential setting), and your research questions.



# The Data
Where does the data come from? How many observations? How many variables? What does each observation refer to (what is the observational unit)? What sorts of data processing was necessary to get the data in shape for analysis?

The data used in this project comes from the Yelp Open Dataset. This is a massive dataset Yelp releases for educational purposes, which is comprised of three main sub datasets: reviews, business, and images. In this project we use the business dataset, a dataset which contains a variety of descriptive variables for 192,609 business location. However, in this project we're focused specifically on restaurants, so when we trim the data to only include restaurants we end up with 59,371 total observations. Each observation is a given restaurant with all of the associated variables, from name of the restaurant, star rating through whether the restaurant serves alcohol. There are a total of 53 variables.

In order to perform any meaningful analysis we needed to clean the data. Thus, as mentioned previously, we began this process by cutting the data to only include restaurants. This resulted in a total of 59,371 observations. We then filtered the data set to only include predictor variables we believed would have the strongest predictive capability- such as: # of stars, whether the restaurant served alcohol, etc.- and proceeded to further cut the data to only include observations which were complete (included an observation for every variable). This resulted in a dataset with 40,584 observations, a loss of about 30% of original observations. We have no reason to believe there exist systematic reasons as to why restaurants would omit data on these variables, and thus chose this as our final dataset, which we use to perform our analysis and create our models.


# Exploratory Data Analysis
Explore the structure of the data through graphics. Here you can utilize both traditional plots as well as methods from unsupervised learning. Understanding the distribution of your response is particular important, but also investigate bivariate and higher-order relationships that you expect to be particular interesting.

In this section we look over the data in order to identify trends, concerns, and other consideration we might need to take into account when creating our models.

## Missingness
First, we looked at the missingness in the data.

```{r, cache = TRUE}
library(ggplot2)
library(Amelia)

## Missingness when including all of the restaurant observations
missmap(rdata)

## Missingness in categorical variables
alcohol <- data.frame(rdata$Alcohol)
outdoorseating <- data.frame(rdata$OutdoorSeating)
dresscode <- data.frame(rdata$RestaurantsAttire)
missmap(outdoorseating)
missmap(alcohol)
missmap(dresscode)

## Missingness when this is removed (should be 0%(it is))
##missmap(rdata.clean)
```

Now that we have a cursory look at the overall data, we turn to preparing it for a survey of the response and predictive variables.

## Preparing Data
```{r}
library(rlist)
library(dplyr)
library(tidyverse)
library(jsonlite)

#business <- stream_in(file("business.json"))
clean.business <- business

## Filtering for restaurants
clean.business$Restaurant <- 0 
new.business <- mutate(clean.business, Restaurant=grepl("Restaurants", clean.business$categories))
restaurants <- filter(new.business, Restaurant == "TRUE")

## Isolating the important variables
attribute <- restaurants$attributes
restaurant_data <- restaurants
restaurant_data$attributes <- NULL
restaurant_data$Restaurant <- NULL
restaurant_data <- cbind(restaurant_data, attribute)
voi <- c("name", "city", "state", "latitude", "longitude", "stars", "review_count", "RestaurantsTakeOut", 
         "RestaurantsPriceRange2", "OutdoorSeating", "Alcohol", "categories", "RestaurantsAttire")
rdata <- subset(restaurant_data, select=voi)

##Cleaning Data to only include complete observations
rdata.clean <- rdata[complete.cases(rdata),]

## Fixing Variables
rdata.clean$OutdoorSeating <- as.logical(rdata.clean$OutdoorSeating)
rdata.clean$OutdoorSeating <- as.numeric(rdata.clean$OutdoorSeating)

rdata.clean$RestaurantsTakeOut <- as.logical(rdata.clean$RestaurantsTakeOut)
rdata.clean$RestaurantsTakeOut <- as.numeric(rdata.clean$RestaurantsTakeOut)

#Assigning categorical levels 1, 2, or 3 for alcohol
rdata.clean$Alc <- 0 
lvlone <- c("'none'", "u'none'")
lvltwo <- c("'beer_and_wine'", "u'beer_and_wine'")
lvlthree <- c( "'full_bar'", "u'full_bar'")
rdata.clean$Alc <- ordered(rdata.clean$Alcohol, levels = lvlone, lvltwo, lvlthree)
rdata.clean$Alc <- as.numeric(rdata.clean$Alc)
rdata.clean$Alc <- rdata.clean$Alc %>% replace_na(3)
```

Thus, here we have our final data set which we use to analyze the response & predictor variables, as well as for the construction of our models.

## Response Variable Analysis
```{r}

ggplot(rdata.clean, aes(x=RestaurantsPriceRange2)) + 
  geom_histogram(color="black", fill="white")

```


## Predictors
```{r}

ggplot(rdata.clean, aes(x=stars)) + 
  geom_histogram(color="black", fill="white")

```



# Modeling
Construct (descriptive and/or predictive) (classification and/or regression) models that address your research questions. You are encouraged to fit many different classes of models and see how they compare in terms the bias/variance tradeoff (do you have a Rashomon effect going on?). Also be sure to guard against overfitting through cross-validation or shrinkage/penalization (don’t forget about ridge regression and the lasso).

This will be the most extensive section and will include your results as well.

## Preparing the Data: Training and Test

```{r}
library(tidyverse)
#class <- ordered(rdata.clean$RestaurantsPriceRange2, levels= c("1", "2", "3", "4")) 
#rdata.clean$class <- class
s.size <- floor(0.75 * nrow(rdata.clean))
set.seed(10)
train.data <- sample(seq_len(nrow(rdata.clean)), size = s.size)
train <- rdata.clean[train.data, ]
test <- rdata.clean[-train.data, ]
```

## Linear Model

```{r}
library(rcompanion)

lm.train <- train[complete.cases(train),]
lm.train$RestaurantsPriceRange2 <- as.numeric(lm.train$RestaurantsPriceRange2)
lm.train$RestaurantsPriceRange2 <- lm.train$RestaurantsPriceRange2
lin.mod <- lm(RestaurantsPriceRange2 ~ stars + review_count + latitude + 
                longitude + RestaurantsAttire +
                RestaurantsTakeOut + OutdoorSeating + Alc,  data = lm.train)

yt <- test$RestaurantsPriceRange2

## Linear Model w/o Rounding

## Predicting Price
predicted.price <- predict(lin.mod, newdata = test)
yp_1 <- predicted.price

## Misclassification rate:
table <- data.frame(yp_1,yt)
mcr.table <- table(yp_1,yt)

1-sum(diag(mcr.table))/sum(mcr.table)

ggplot(table, aes(x=yt, y=yp)) + 
  geom_point()+
  labs(title="Predicted v Actual: Linear Model",
       x="Actual Price Range", y = "Predicted Price Range") +
  theme_classic()


## Linear Model w/o rounding
predicted.price.rounded <- round(predicted.price, digits = 0)
yp <- predicted.price.rounded

table_2 <- data.frame(predicted.price.rounded, yt)
mcr2 <- table(predicted.price.rounded,yt)
1-sum(diag(mcr2))/sum(mcr2)

ggplot(table_2, aes(x=yt, y=predicted.price.rounded)) + 
  geom_point()+
  labs(title="Predicted v Actual: Linear Model (W/ Rounding)",
       x="Actual Price Range", y = "Predicted Price Range") +
  theme_classic()

```



# Discussion
Review the results generated above and sythensize them in the context from which the data originated. What do the results tell your about your original research question? Are there any weaknesses that you see in your analysis? What additional questions would you explore next?



# References
At minimum, this will contain the full citation for your data set. If you reference existing analyses, they should be cited here as well.
