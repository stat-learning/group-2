---
title: "Technical Report: Predicting Yelp Price Ranges"
author:
-"Josh Dey"
-"Emmett Powers"
-"Giorlando Ramirez"
date: "12/10/2019"
output: pdf_document
---

***

# Abstract
In this project, we leverage a large Yelp data set comprised of about 200k observations in order to build a predictive model of restaurant prices. We go about answering this question deploying three supervised learning models: linear regression, ordinal regression, and classification trees. We find that the available variables have fairly limited predictive power across our models.

### Preparing Libraries
```{r}
library(ggplot2)
library(Amelia)
library(rlist)
library(dplyr)
library(tidyverse)
library(jsonlite)
library(rcompanion)
```


# Introduction
In this project we want to use a large dataset provided by Yelp to predict restaurant prices. Economic intuition would have it that there are certain things restaurants might sell and/or provide that would make them more or less expensive, a prime example being the sale of alcohol. Thus, with this project we wish to put this intuition to the test. The Yelp dataset is comprised of many individual restaurant observations with variables that are likely determinants of the restaurants prices, such as whether they provide outdoor seating or the expected attire, so with this data we go about building a variety of models which might predict a given restaurant's price range.


# The Data
The data used in this project comes from the Yelp Open Dataset. This is a massive dataset Yelp releases for educational purposes, which is comprised of three main sub datasets: reviews, business, and images. In this project we use the business dataset, a dataset which contains a variety of descriptive variables for 192,609 business location. However, in this project we're focused specifically on restaurants, so when we trim the data to only include restaurants we end up with 59,371 total observations. Each observation is a given restaurant with all of the associated variables, from name of the restaurant, star rating through whether the restaurant serves alcohol. There are a total of 53 variables.

In order to perform any meaningful analysis we needed to clean the data. Thus, as mentioned previously, we began this process by cutting the data to only include restaurants. This resulted in a total of 59,371 observations. We then filtered the data set to only include predictor variables we believed would have the strongest predictive capability- such as: # of stars, whether the restaurant served alcohol, etc.- and proceeded to further cut the data to only include observations which were complete (included an observation for every variable). This resulted in a dataset with 40,584 observations, a loss of about 30% of original observations. We have no reason to believe there exist systematic reasons as to why restaurants would omit data on these variables, and thus chose this as our final dataset, which we use to perform our analysis and create our models.


# Exploratory Data Analysis
Explore the structure of the data through graphics. Here you can utilize both traditional plots as well as methods from unsupervised learning. Understanding the distribution of your response is particular important, but also investigate bivariate and higher-order relationships that you expect to be particular interesting.

In this section we look over the data in order to identify trends, concerns, and other consideration we might need to take into account when creating our models.

### Initial Data Prep
First, we load up the data before we begin our exploratory analysis. 
We also filter the data for the explanatory variables we wish to use.

```{r}
#business <- stream_in(file("business.json"))
clean.business <- business

## Filtering for restaurants
clean.business$Restaurant <- 0 
new.business <- mutate(clean.business, Restaurant=grepl("Restaurants", clean.business$categories))
restaurants <- filter(new.business, Restaurant == "TRUE")

## Isolating the important variables
attribute <- restaurants$attributes
restaurant_data <- restaurants
restaurant_data$attributes <- NULL
restaurant_data$Restaurant <- NULL
restaurant_data <- cbind(restaurant_data, attribute)
voi <- c("name", "city", "state", "latitude", "longitude", "stars", "review_count", "RestaurantsTakeOut", 
         "RestaurantsPriceRange2", "OutdoorSeating", "Alcohol", "categories", "RestaurantsAttire")
rdata <- subset(restaurant_data, select=voi)
```

## Missingness
Now we look at the missingness in the data, and we decouple it into response and some predictor variables.

```{r, cache = TRUE}
## Missingness when including all of the restaurant observations
missmap(rdata)

## Missingness in the response
prices <- data.frame(rdata$RestaurantsPriceRange2)
missmap(prices)

## Missingness in the explanatory variables
alcohol <- data.frame(rdata$Alcohol)
outdoorseating <- data.frame(rdata$OutdoorSeating)
dresscode <- data.frame(rdata$RestaurantsAttire)
missmap(outdoorseating)
missmap(alcohol)
missmap(dresscode)
```

The missingness maps reveal that though our overall missingness is minimal, there are a couple things worth noting. In terms of the response variable, it is only 12% which is not too concerning since we still have a value for the major of observations. Further, we notice that the missingness is concentrated around the more niche variables, which are the determinants of the price range. This would be an issue if we considered there to exist a systematic issue in the missingness occurring, however we do not think this is the case. We believe the missingness is randomly distributed, so we assume it is not problematic to simply get rid of observations which do not have values for all of our variables. Thus, the final data we use will contain only complete observations. Now that we have a cursory look at the overall data and know that we will get rid of incomplete observations, we turn to preparing it for a survey of the response and predictive variables.

## Preparing Data
```{r}
##Cleaning Data to only include complete observations
rdata.clean <- rdata[complete.cases(rdata),]

## Fixing Variables
rdata.clean$OutdoorSeating <- as.logical(rdata.clean$OutdoorSeating)
rdata.clean$OutdoorSeating <- as.numeric(rdata.clean$OutdoorSeating)

rdata.clean$RestaurantsTakeOut <- as.logical(rdata.clean$RestaurantsTakeOut)
rdata.clean$RestaurantsTakeOut <- as.numeric(rdata.clean$RestaurantsTakeOut)

#Assigning categorical levels 1, 2, or 3 for alcohol
rdata.clean$Alc <- 0 
lvlone <- c("'none'", "u'none'")
lvltwo <- c("'beer_and_wine'", "u'beer_and_wine'")
lvlthree <- c( "'full_bar'", "u'full_bar'")
rdata.clean$Alc <- ordered(rdata.clean$Alcohol, levels = c(lvlone, lvltwo, lvlthree))
rdata.clean$Alc <- as.numeric(rdata.clean$Alc)
rdata.clean$Alc[rdata.clean$Alc == 2] <- 1
rdata.clean$Alc[rdata.clean$Alc == 3] <- 2
rdata.clean$Alc[rdata.clean$Alc == 4] <- 2
rdata.clean$Alc[rdata.clean$Alc == 5] <- 3
rdata.clean$Alc[rdata.clean$Alc == 6] <- 3

# Doing the same for Attire; had to do it differently for some reason
rdata.clean$RestaurantsAttire[rdata.clean$RestaurantsAttire == "'casual'"] <- 1
rdata.clean$RestaurantsAttire[rdata.clean$RestaurantsAttire == "u'casual'"] <- 1
rdata.clean$RestaurantsAttire[rdata.clean$RestaurantsAttire == "'dressy'"] <- 2
rdata.clean$RestaurantsAttire[rdata.clean$RestaurantsAttire == "u'dressy'"] <- 2
rdata.clean$RestaurantsAttire[rdata.clean$RestaurantsAttire == "'formal'"] <- 3
rdata.clean$RestaurantsAttire[rdata.clean$RestaurantsAttire == "u'formal'"] <- 3

rdata.cleaner <- na.omit(rdata.clean)

d1 <- rdata.cleaner[c(-1 , -2, -3, -11, -12 )]

```

Now we have d1, our final data set which we use to analyze the response & predictor variables, as well as for the construction of our models.

## Response Variable Analysis
```{r}
d1$RestaurantsPriceRange2 <- as.numeric(d1$RestaurantsPriceRange2)
ggplot(d1, aes(x=RestaurantsPriceRange2)) + 
  geom_histogram(color="black", fill="white") +
  xlab("Restaurant Rrice Range (# of $)") +
  ylab("Number of Restaurants in Range")
```


## Predictors
```{r}
ggplot(d1, aes(x=stars)) + 
  geom_histogram(color="black", fill="white")

ggplot(d1, aes(x=Alc)) + 
  geom_histogram(color="black", fill="white")
```


# Modeling
Construct (descriptive and/or predictive) (classification and/or regression) models that address your research questions. You are encouraged to fit many different classes of models and see how they compare in terms the bias/variance tradeoff (do you have a Rashomon effect going on?). Also be sure to guard against overfitting through cross-validation or shrinkage/penalization (don’t forget about ridge regression and the lasso).

This will be the most extensive section and will include your results as well.

## Preparing the Data: Training and Test

Before building our supervised learning models, we must first create our training and test sub datasets.

```{r}
#class <- ordered(d1$RestaurantsPriceRange2, levels= c("1", "2", "3", "4")) 
#d1$class <- class
s.size <- floor(0.75 * nrow(d1))
set.seed(10)
train.data <- sample(seq_len(nrow(d1)), size = s.size)
train <- d1[train.data, ]
test <- d1[-train.data, ]
```

## Linear Model

Though this is a classification exercise, the first model we developed was a linear model with rounding, as follows:

```{r}
lm.train <- train[complete.cases(train),]
lm.train$RestaurantsPriceRange2 <- as.numeric(lm.train$RestaurantsPriceRange2)
lin.mod <- lm(RestaurantsPriceRange2 ~ stars + review_count + latitude + 
                longitude + RestaurantsAttire +
                RestaurantsTakeOut + OutdoorSeating + Alc,  data = lm.train)

## Predicting Price
predicted.price <- predict(lin.mod, newdata = test)
predicted.price.rounded <- round(predicted.price, digits = 0)
yp <- predicted.price.rounded

## Misclassification rate:
yt <- test$RestaurantsPriceRange2

mcr <- table(predicted.price.rounded,yt)

1-sum(diag(mcr))/sum(mcr)
```

With this model we find a misclassification rate of 27.62%. While this modeling method is not necessarily made for this exercise, it is quite robust since it identifies just over 70% of restaurants correctly. However, we wished to further develop our understanding of the relationship between restaurant prices and the variables, so we continue our explorations with an Ordinal Regression, which should more aptly tackle the classification nature of this problem.

## Ordignal Regression

## Classification Tree


# Discussion
Review the results generated above and sythensize them in the context from which the data originated. What do the results tell your about your original research question? Are there any weaknesses that you see in your analysis? What additional questions would you explore next?

# References

“Yelp Open Dataset.” Yelp Dataset, 2019, www.yelp.com/dataset.
